{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hate Speech Detection - MLOps Project","text":""},{"location":"source/my_api/","title":"MLOps Hate Speech Project Documentation","text":"<p>Documentation of core classes and functions used in this project.</p>"},{"location":"source/my_api/#data-preparation","title":"Data Preparation","text":""},{"location":"source/my_api/#load_and_prepare_dataset","title":"load_and_prepare_dataset","text":"<p>Loads, merges, splits, and saves the hate speech dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the processed dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/mlops_hatespeech/data.py</code> <pre><code>def load_and_prepare_dataset(seed: int = 42, save_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Loads, merges, splits, and saves the hate speech dataset.\n\n    Args:\n        seed (int): Random seed for shuffling the dataset.\n        save_path (Optional[str]): Path to save the processed dataset.\n        If None, uses default path under /data/processed.\n\n    Returns:\n        None\n    \"\"\"\n    if save_path is None:\n        save_path = DEFAULT_SAVE_PATH\n    else:\n        save_path = Path(save_path)\n\n    # Load original dataset\n    ds = load_dataset(\"thefrankhsu/hate_speech_twitter\")\n\n    # Concatenate train and test into one dataset\n    combined = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n\n    # Shuffle for good measure\n    combined = combined.shuffle(seed=seed)\n\n    # Split into train (70%), val (15%), test (15%)\n    n = len(combined)\n    train_size = int(0.7 * n)\n    val_size = int(0.15 * n)\n\n    train_ds = combined.select(range(0, train_size))\n    val_ds = combined.select(range(train_size, train_size + val_size))\n    test_ds = combined.select(range(train_size + val_size, n))\n\n    # Combine into DatasetDict and save\n    full_dataset = DatasetDict(\n        {\n            \"train\": train_ds,\n            \"validation\": val_ds,\n            \"test\": test_ds,\n        }\n    )\n\n    # Concatenate train and test into one dataset\n    combined = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n\n    # Shuffle for good measure\n    combined = combined.shuffle(seed=seed)\n\n    # Split into train (70%), val (15%), test (15%)\n    n = len(combined)\n    train_size = int(0.7 * n)\n    val_size = int(0.15 * n)\n\n    train_ds = combined.select(range(0, train_size))\n    val_ds = combined.select(range(train_size, train_size + val_size))\n    test_ds = combined.select(range(train_size + val_size, n))\n\n    # Combine into DatasetDict and save\n    full_dataset = DatasetDict(\n        {\n            \"train\": train_ds,\n            \"validation\": val_ds,\n            \"test\": test_ds,\n        }\n    )\n\n    full_dataset.save_to_disk(str(save_path))\n    print(f\"Dataset saved to {save_path}\")\n</code></pre>"},{"location":"source/my_api/#model-training","title":"Model Training","text":""},{"location":"source/my_api/#train_model","title":"train_model","text":"<p>Load configuration using Hydra with optional overrides.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>Optional[List[str]]</code> <p>List of override strings</p> required <p>Returns:</p> Name Type Description <code>DictConfig</code> <code>Trainer</code> <p>Composed configuration object.</p> Source code in <code>src/mlops_hatespeech/train.py</code> <pre><code>def train_model(cfg: DictConfig) -&gt; Trainer:\n    \"\"\"\n    Load configuration using Hydra with optional overrides.\n\n    Args:\n        overrides (Optional[List[str]]): List of override strings\n\n    Returns:\n        DictConfig: Composed configuration object.\n    \"\"\"\n    logger.info(f\"Loading dataset from: {cfg.data_path}\")\n    ds = load_from_disk(cfg.data_path)\n\n    idx2lbl = {\n        0: \"non-hate\",\n        1: \"hate\",\n    }\n    lbl2idx = {v: k for k, v in idx2lbl.items()}\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_STR)\n\n    def tokenize_seqs(examples):\n        texts = examples[\"tweet\"]\n        return tokenizer(texts, truncation=True, max_length=512)\n\n    def is_valid(example):\n        text = example[\"tweet\"]\n        return isinstance(text, str) and len(text.strip()) &gt; 0\n\n    ds = ds.filter(is_valid)\n    ds = ds.map(tokenize_seqs, batched=True)\n    ds = ds.rename_column(\"label\", \"labels\")\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_STR,\n        num_labels=len(lbl2idx),\n        id2label=idx2lbl,\n        label2id=lbl2idx,\n    )\n\n    def compute_metrics(eval_preds):\n        logits, labels = eval_preds.predictions, eval_preds.label_ids\n        pred_labels = np.argmax(logits, axis=-1)\n\n        f1 = f1_score(y_true=labels, y_pred=pred_labels, average=\"weighted\")\n        acc = accuracy_score(y_true=labels, y_pred=pred_labels)\n\n        return {\"f1\": f1, \"accuracy\": acc}\n\n    training_args = TrainingArguments(\n        output_dir=\"./logs/run1\",\n        per_device_train_batch_size=cfg.hyperparameters.per_device_train_batch_size,\n        per_gpu_eval_batch_size=cfg.hyperparameters.per_gpu_eval_batch_size,\n        gradient_accumulation_steps=cfg.hyperparameters.gradient_accumulation_steps,\n        learning_rate=cfg.hyperparameters.lr,\n        weight_decay=cfg.hyperparameters.wd,\n        num_train_epochs=cfg.hyperparameters.epochs,\n        logging_strategy=cfg.hyperparameters.logging_strategy,\n        logging_steps=cfg.hyperparameters.logging_steps,\n        save_strategy=cfg.hyperparameters.save_strategy,\n        eval_strategy=cfg.hyperparameters.eval_strategy,\n        eval_steps=cfg.hyperparameters.eval_steps,\n        save_total_limit=cfg.hyperparameters.save_total_limit,\n        seed=cfg.hyperparameters.seed,\n        data_seed=cfg.hyperparameters.seed,\n        dataloader_num_workers=cfg.hyperparameters.dataloader_num_workers,\n        load_best_model_at_end=cfg.hyperparameters.load_best_model_at_end,\n        report_to=cfg.hyperparameters.report_to,\n        use_cpu=cfg.hyperparameters.use_cpu,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=ds[\"train\"],\n        eval_dataset=ds[\"validation\"],\n        tokenizer=tokenizer,\n    )\n\n    trainer.train()\n\n    preds_output = trainer.predict(ds[\"validation\"])\n    preds_probs = preds_output.predictions\n    labels = preds_output.label_ids\n\n    RocCurveDisplay.from_predictions(\n        labels,\n        preds_probs[:, 1],\n        name=\"ROC Curve\",\n    )\n\n    wandb.log({\"roc_curve\": wandb.Image(plt.gcf())})\n    plt.close()\n\n    metrics = trainer.evaluate()\n\n    torch.save(model.state_dict(), \"model.pth\")\n    artifact = wandb.Artifact(\n        name=\"mlops_hatespeech_model\",\n        type=\"model\",\n        description=\"A model trained to detect hate speech in tweets.\",\n        metadata=metrics,\n    )\n    artifact.add_file(\"model.pth\")\n    wandb.log_artifact(artifact)\n\n    return trainer\n</code></pre>"},{"location":"source/my_api/#train","title":"train","text":"<p>Entry point for training. Optionally override key hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>Optional[float]</code> <p>Learning rate.</p> <code>None</code> <code>wd</code> <code>Optional[float]</code> <p>Weight decay.</p> <code>None</code> <code>epochs</code> <code>Optional[int]</code> <p>Number of training epochs.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <code>None</code> Source code in <code>src/mlops_hatespeech/train.py</code> <pre><code>@app.command()\ndef train(\n    lr: Optional[float] = None, wd: Optional[float] = None, epochs: Optional[int] = None, seed: Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Entry point for training. Optionally override key hyperparameters.\n\n    Args:\n        lr (Optional[float]): Learning rate.\n        wd (Optional[float]): Weight decay.\n        epochs (Optional[int]): Number of training epochs.\n        seed (Optional[int]): Random seed.\n    \"\"\"\n    overrides = []\n    if lr is not None:\n        overrides.append(f\"hyperparameters.lr={lr}\")\n    if wd is not None:\n        overrides.append(f\"hyperparameters.wd={wd}\")\n    if epochs is not None:\n        overrides.append(f\"hyperparameters.epochs={epochs}\")\n    if seed is not None:\n        overrides.append(f\"hyperparameters.seed={seed}\")\n\n    cfg = get_config(overrides)\n\n    # wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True)\n\n    wandb.init(\n        project=\"mlops_hatespeech\",\n        config={\n            \"learning rate\": cfg.hyperparameters.lr,\n            \"weight decay\": cfg.hyperparameters.wd,\n            \"epochs\": cfg.hyperparameters.epochs,\n            \"model\": MODEL_STR,\n        },\n    )\n\n    # Run the actual training\n    trainer = train_model(cfg)\n    logger.info(\"Training is done.\")\n\n    wandb.finish()\n</code></pre>"},{"location":"source/my_api/#model-evaluation","title":"Model Evaluation","text":""},{"location":"source/my_api/#find_latest_checkpoint","title":"find_latest_checkpoint","text":"<p>Finds the latest checkpoint folder in the given directory based on the highest checkpoint number.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>str</code> <p>Path to the directory containing checkpoint folders.</p> <code>'logs/run1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the latest checkpoint folder.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no checkpoints are found.</p> Source code in <code>src/mlops_hatespeech/evaluate.py</code> <pre><code>def find_latest_checkpoint(run_dir: str = \"logs/run1\") -&gt; str:\n    \"\"\"\n    Finds the latest checkpoint folder in the given directory based on the highest checkpoint number.\n\n    Args:\n        run_dir (str): Path to the directory containing checkpoint folders.\n\n    Returns:\n        str: Path to the latest checkpoint folder.\n\n    Raises:\n        FileNotFoundError: If no checkpoints are found.\n    \"\"\"\n    checkpoints = []\n    pattern = re.compile(r\"^checkpoint-(\\d+)$\")\n    for name in os.listdir(run_dir):\n        match = pattern.match(name)\n        if match:\n            checkpoints.append((int(match.group(1)), name))\n\n    if not checkpoints:\n        raise FileNotFoundError(f\"No training checkpoints found.\")\n\n    # Pick highest\n    latest_checkpoint = max(checkpoints, key=lambda x: x[0])[1]\n    return os.path.join(run_dir, latest_checkpoint)\n</code></pre>"},{"location":"source/my_api/#compute_metrics","title":"compute_metrics","text":"<p>Computes accuracy and weighted F1 score from model predictions and true labels.</p> <p>Parameters:</p> Name Type Description Default <code>eval_preds</code> <code>Any</code> <p>An object with 'predictions' (logits) and 'label_ids' (true labels).</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary with 'f1' and 'accuracy' scores.</p> Source code in <code>src/mlops_hatespeech/evaluate.py</code> <pre><code>def compute_metrics(eval_preds: Any) -&gt; Dict[str, float]:\n    \"\"\"\n    Computes accuracy and weighted F1 score from model predictions and true labels.\n\n    Args:\n        eval_preds: An object with 'predictions' (logits) and 'label_ids' (true labels).\n\n    Returns:\n        Dict[str, float]: Dictionary with 'f1' and 'accuracy' scores.\n    \"\"\"\n    logits, labels = eval_preds.predictions, eval_preds.label_ids\n    pred_labels = np.argmax(logits, axis=-1)\n    f1 = f1_score(y_true=labels, y_pred=pred_labels, average=\"weighted\")\n    acc = accuracy_score(y_true=labels, y_pred=pred_labels)\n    return {\n        \"f1\": f1,\n        \"accuracy\": acc,\n    }\n</code></pre>"}]}