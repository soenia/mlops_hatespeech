{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hate Speech Detection - MLOps Project","text":""},{"location":"source/docs/","title":"MLOps Hate Speech Project Documentation","text":"<p>Documentation of core classes and functions used in this project.</p>"},{"location":"source/docs/#data-preparation","title":"Data Preparation","text":""},{"location":"source/docs/#load_and_prepare_dataset","title":"load_and_prepare_dataset","text":"<p>Loads, merges, splits, and saves the hate speech dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the processed dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/mlops_hatespeech/data.py</code> <pre><code>def load_and_prepare_dataset(seed: int = 42, save_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Loads, merges, splits, and saves the hate speech dataset.\n\n    Args:\n        seed (int): Random seed for shuffling the dataset.\n        save_path (Optional[str]): Path to save the processed dataset.\n        If None, uses default path under /data/processed.\n\n    Returns:\n        None\n    \"\"\"\n    if save_path is None:\n        save_path = DEFAULT_SAVE_PATH\n    else:\n        save_path = Path(save_path)\n\n    # Load original dataset\n    ds = load_dataset(\"thefrankhsu/hate_speech_twitter\")\n\n    # Concatenate train and test into one dataset\n    combined = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n\n    # Shuffle for good measure\n    combined = combined.shuffle(seed=seed)\n\n    # Split into train (70%), val (15%), test (15%)\n    n = len(combined)\n    train_size = int(0.7 * n)\n    val_size = int(0.15 * n)\n\n    train_ds = combined.select(range(0, train_size))\n    val_ds = combined.select(range(train_size, train_size + val_size))\n    test_ds = combined.select(range(train_size + val_size, n))\n\n    # Combine into DatasetDict and save\n    full_dataset = DatasetDict(\n        {\n            \"train\": train_ds,\n            \"validation\": val_ds,\n            \"test\": test_ds,\n        }\n    )\n\n    # Concatenate train and test into one dataset\n    combined = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n\n    # Shuffle for good measure\n    combined = combined.shuffle(seed=seed)\n\n    # Split into train (70%), val (15%), test (15%)\n    n = len(combined)\n    train_size = int(0.7 * n)\n    val_size = int(0.15 * n)\n\n    train_ds = combined.select(range(0, train_size))\n    val_ds = combined.select(range(train_size, train_size + val_size))\n    test_ds = combined.select(range(train_size + val_size, n))\n\n    # Combine into DatasetDict and save\n    full_dataset = DatasetDict(\n        {\n            \"train\": train_ds,\n            \"validation\": val_ds,\n            \"test\": test_ds,\n        }\n    )\n\n    full_dataset.save_to_disk(str(save_path))\n    print(f\"Dataset saved to {save_path}\")\n</code></pre>"},{"location":"source/docs/#model-training","title":"Model Training","text":""},{"location":"source/docs/#get_config","title":"get_config","text":"<p>Get the configuration from Hydra.</p> Source code in <code>src/mlops_hatespeech/train.py</code> <pre><code>def get_config(overrides: Optional[List[str]]) -&gt; DictConfig:\n    \"\"\"Get the configuration from Hydra.\"\"\"\n    with initialize(config_path=\"../..\", job_name=\"train_app\", version_base=\"1.1\"):\n        return compose(config_name=\"config\", overrides=overrides or [])\n</code></pre>"},{"location":"source/docs/#train_model","title":"train_model","text":"<p>Load configuration using Hydra with optional overrides.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>Optional[List[str]]</code> <p>List of override strings</p> required <p>Returns:</p> Name Type Description <code>DictConfig</code> <code>Trainer</code> <p>Composed configuration object.</p> Source code in <code>src/mlops_hatespeech/train.py</code> <pre><code>def train_model(cfg: DictConfig) -&gt; Trainer:\n    \"\"\"\n    Load configuration using Hydra with optional overrides.\n\n    Args:\n        overrides (Optional[List[str]]): List of override strings\n\n    Returns:\n        DictConfig: Composed configuration object.\n    \"\"\"\n    logger.info(f\"Loading dataset from: {cfg.data_path}\")\n    ds = load_from_disk(cfg.data_path)\n\n    idx2lbl = {\n        0: \"non-hate\",\n        1: \"hate\",\n    }\n    lbl2idx = {v: k for k, v in idx2lbl.items()}\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_STR)\n\n    def tokenize_seqs(examples):\n        texts = examples[\"tweet\"]\n        return tokenizer(texts, truncation=True, max_length=512)\n\n    def is_valid(example):\n        text = example[\"tweet\"]\n        return isinstance(text, str) and len(text.strip()) &gt; 0\n\n    ds = ds.filter(is_valid)\n    ds = ds.map(tokenize_seqs, batched=True)\n    ds = ds.rename_column(\"label\", \"labels\")\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_STR,\n        num_labels=len(lbl2idx),\n        id2label=idx2lbl,\n        label2id=lbl2idx,\n    )\n\n    def compute_metrics(eval_preds):\n        logits, labels = eval_preds.predictions, eval_preds.label_ids\n        pred_labels = np.argmax(logits, axis=-1)\n\n        f1 = f1_score(y_true=labels, y_pred=pred_labels, average=\"weighted\")\n        acc = accuracy_score(y_true=labels, y_pred=pred_labels)\n\n        return {\"f1\": f1, \"accuracy\": acc}\n\n    training_args = TrainingArguments(\n        output_dir=\"./logs/run1\",\n        per_device_train_batch_size=cfg.hyperparameters.per_device_train_batch_size,\n        per_gpu_eval_batch_size=cfg.hyperparameters.per_gpu_eval_batch_size,\n        gradient_accumulation_steps=cfg.hyperparameters.gradient_accumulation_steps,\n        learning_rate=cfg.hyperparameters.lr,\n        weight_decay=cfg.hyperparameters.wd,\n        num_train_epochs=cfg.hyperparameters.epochs,\n        logging_strategy=cfg.hyperparameters.logging_strategy,\n        logging_steps=cfg.hyperparameters.logging_steps,\n        save_strategy=cfg.hyperparameters.save_strategy,\n        eval_strategy=cfg.hyperparameters.eval_strategy,\n        eval_steps=cfg.hyperparameters.eval_steps,\n        save_total_limit=cfg.hyperparameters.save_total_limit,\n        seed=cfg.hyperparameters.seed,\n        data_seed=cfg.hyperparameters.seed,\n        dataloader_num_workers=cfg.hyperparameters.dataloader_num_workers,\n        load_best_model_at_end=cfg.hyperparameters.load_best_model_at_end,\n        report_to=cfg.hyperparameters.report_to,\n        use_cpu=cfg.hyperparameters.use_cpu,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=ds[\"train\"],\n        eval_dataset=ds[\"validation\"],\n        tokenizer=tokenizer,\n    )\n\n    trainer.train()\n\n    preds_output = trainer.predict(ds[\"validation\"])\n    preds_probs = preds_output.predictions\n    labels = preds_output.label_ids\n\n    RocCurveDisplay.from_predictions(\n        labels,\n        preds_probs[:, 1],\n        name=\"ROC Curve\",\n    )\n\n    wandb.log({\"roc_curve\": wandb.Image(plt.gcf())})\n    plt.close()\n\n    metrics = trainer.evaluate()\n\n    torch.save(model.state_dict(), \"model.pth\")\n    artifact = wandb.Artifact(\n        name=\"mlops_hatespeech_model\",\n        type=\"model\",\n        description=\"A model trained to detect hate speech in tweets.\",\n        metadata=metrics,\n    )\n    artifact.add_file(\"model.pth\")\n    wandb.log_artifact(artifact)\n\n    return trainer\n</code></pre>"},{"location":"source/docs/#train","title":"train","text":"<p>Entry point for training. Optionally override key hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>Optional[float]</code> <p>Learning rate.</p> <code>None</code> <code>wd</code> <code>Optional[float]</code> <p>Weight decay.</p> <code>None</code> <code>epochs</code> <code>Optional[int]</code> <p>Number of training epochs.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <code>None</code> Source code in <code>src/mlops_hatespeech/train.py</code> <pre><code>@app.command()\ndef train(\n    lr: Optional[float] = None, wd: Optional[float] = None, epochs: Optional[int] = None, seed: Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Entry point for training. Optionally override key hyperparameters.\n\n    Args:\n        lr (Optional[float]): Learning rate.\n        wd (Optional[float]): Weight decay.\n        epochs (Optional[int]): Number of training epochs.\n        seed (Optional[int]): Random seed.\n    \"\"\"\n    overrides = []\n    if lr is not None:\n        overrides.append(f\"hyperparameters.lr={lr}\")\n    if wd is not None:\n        overrides.append(f\"hyperparameters.wd={wd}\")\n    if epochs is not None:\n        overrides.append(f\"hyperparameters.epochs={epochs}\")\n    if seed is not None:\n        overrides.append(f\"hyperparameters.seed={seed}\")\n\n    cfg = get_config(overrides)\n\n    # wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True)\n\n    wandb.init(\n        project=\"mlops_hatespeech\",\n        config={\n            \"learning rate\": cfg.hyperparameters.lr,\n            \"weight decay\": cfg.hyperparameters.wd,\n            \"epochs\": cfg.hyperparameters.epochs,\n            \"model\": MODEL_STR,\n        },\n    )\n\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Run the actual training\n    trainer = train_model(cfg)\n\n    profiler.disable()\n    s = io.StringIO()\n    ps = pstats.Stats(profiler, stream=s).sort_stats(\"cumtime\")\n    ps.print_stats()\n\n    with open(\"reports/logs/train_profile.txt\", \"w\") as f:\n        f.write(s.getvalue())\n\n    logger.info(\"Training is done.\")\n\n    wandb.finish()\n</code></pre>"},{"location":"source/docs/#model-evaluation","title":"Model Evaluation","text":""},{"location":"source/docs/#find_latest_checkpoint","title":"find_latest_checkpoint","text":"<p>Finds the latest checkpoint folder in the given directory based on the highest checkpoint number.</p> <p>Parameters:</p> Name Type Description Default <code>run_dir</code> <code>str</code> <p>Path to the directory containing checkpoint folders.</p> <code>'logs/run1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the latest checkpoint folder.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no checkpoints are found.</p> Source code in <code>src/mlops_hatespeech/evaluate.py</code> <pre><code>def find_latest_checkpoint(run_dir: str = \"logs/run1\") -&gt; str:\n    \"\"\"\n    Finds the latest checkpoint folder in the given directory based on the highest checkpoint number.\n\n    Args:\n        run_dir (str): Path to the directory containing checkpoint folders.\n\n    Returns:\n        str: Path to the latest checkpoint folder.\n\n    Raises:\n        FileNotFoundError: If no checkpoints are found.\n    \"\"\"\n    checkpoints = []\n    pattern = re.compile(r\"^checkpoint-(\\d+)$\")\n    for name in os.listdir(run_dir):\n        match = pattern.match(name)\n        if match:\n            checkpoints.append((int(match.group(1)), name))\n\n    if not checkpoints:\n        raise FileNotFoundError(f\"No training checkpoints found.\")\n\n    # Pick highest\n    latest_checkpoint = max(checkpoints, key=lambda x: x[0])[1]\n    return os.path.join(run_dir, latest_checkpoint)\n</code></pre>"},{"location":"source/docs/#compute_metrics","title":"compute_metrics","text":"<p>Computes accuracy and weighted F1 score from model predictions and true labels.</p> <p>Parameters:</p> Name Type Description Default <code>eval_preds</code> <code>Any</code> <p>An object with 'predictions' (logits) and 'label_ids' (true labels).</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary with 'f1' and 'accuracy' scores.</p> Source code in <code>src/mlops_hatespeech/evaluate.py</code> <pre><code>def compute_metrics(eval_preds: Any) -&gt; Dict[str, float]:\n    \"\"\"\n    Computes accuracy and weighted F1 score from model predictions and true labels.\n\n    Args:\n        eval_preds: An object with 'predictions' (logits) and 'label_ids' (true labels).\n\n    Returns:\n        Dict[str, float]: Dictionary with 'f1' and 'accuracy' scores.\n    \"\"\"\n    logits, labels = eval_preds.predictions, eval_preds.label_ids\n    pred_labels = np.argmax(logits, axis=-1)\n    f1 = f1_score(y_true=labels, y_pred=pred_labels, average=\"weighted\")\n    acc = accuracy_score(y_true=labels, y_pred=pred_labels)\n    return {\n        \"f1\": f1,\n        \"accuracy\": acc,\n    }\n</code></pre>"},{"location":"source/docs/#main","title":"main","text":"<p>Main function: Loads the latest checkpoint, prepares the dataset, runs evaluation, and uploads results as a JSON file to a GCS bucket.</p> Source code in <code>src/mlops_hatespeech/evaluate.py</code> <pre><code>def main():\n    \"\"\"\n    Main function: Loads the latest checkpoint, prepares the dataset,\n    runs evaluation, and uploads results as a JSON file to a GCS bucket.\n    \"\"\"\n    # Load highest checkpoint\n    checkpoint_path = find_latest_checkpoint()\n\n    # Load data\n    ds = load_from_disk(\"data/processed\")\n\n    def is_valid(example):\n        text = example[\"tweet\"]\n        return isinstance(text, str) and len(text.strip()) &gt; 0\n\n    ds = ds.filter(is_valid)\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_STR)\n\n    # Filter only valid examples where 'tweet' is a non-empty string\n    def tokenize_seqs(examples):\n        return tokenizer(examples[\"tweet\"], truncation=True, max_length=512)\n\n    ds = ds.map(tokenize_seqs, batched=True)\n    ds = ds.rename_column(\"label\", \"labels\")\n\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n\n    training_args = TrainingArguments(\n        output_dir=\"./logs/eval\",\n        per_device_eval_batch_size=128,\n        no_cuda=True,\n        report_to=None,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n    )\n\n    # Run evaluation on the test dataset\n    eval_result = trainer.evaluate(eval_dataset=ds[\"test\"])\n    print(\"Evaluated the thing\")\n    print(eval_result)\n\n    # Save evaluation results as JSON\n    output_path = \"logs/eval/gen_perf.json\"\n    with open(output_path, \"w\") as f:\n        json.dump(eval_result, f, indent=2)\n\n    # Upload results JSON file to GCS bucket\n    client = storage.Client(project=\"mlops-hs-project\")\n    bucket = client.bucket(BUCKET_NAME)\n    blob = bucket.blob(\"logs/eval/gen_perf.json\")\n    blob.upload_from_filename(output_path)\n\n    print(\"Uploaded results to Bucket.\")\n</code></pre>"},{"location":"source/docs/#drift-detection","title":"Drift Detection","text":""},{"location":"source/docs/#get_bert_embeddings","title":"get_bert_embeddings","text":"<p>Generates mean-pooled BERT embeddings for a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>Input texts.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Tokenizer for the model.</p> required <code>model</code> <code>PreTrainedModel</code> <p>Pretrained BERT model.</p> required <code>device</code> <code>str</code> <p>Device to run the model on, default is \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D numpy array with shape (n_texts, hidden_size).</p> Source code in <code>src/mlops_hatespeech/drift_detector.py</code> <pre><code>def get_bert_embeddings(texts: List[str], tokenizer: Any, model: Any, device: str = \"cpu\") -&gt; np.ndarray:\n    \"\"\"\n    Generates mean-pooled BERT embeddings for a list of texts.\n\n    Args:\n        texts (List[str]): Input texts.\n        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for the model.\n        model (transformers.PreTrainedModel): Pretrained BERT model.\n        device (str): Device to run the model on, default is \"cpu\".\n\n    Returns:\n        np.ndarray: 2D numpy array with shape (n_texts, hidden_size).\n    \"\"\"\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    attention_mask = inputs[\"attention_mask\"]\n    token_embeddings = outputs.last_hidden_state\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    summed = torch.sum(token_embeddings * input_mask_expanded, 1)\n    summed_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    mean_pooled = summed / summed_mask\n\n    return mean_pooled.cpu().numpy()\n</code></pre>"},{"location":"source/docs/#download_predictions_from_gcs","title":"download_predictions_from_gcs","text":"<p>Downloads prediction JSON files from a Google Cloud Storage bucket.</p> <p>Each JSON is expected to contain a 'tweet' (under 'input_text') and a 'label' (under 'prediction'). These are collected into a DataFrame for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the GCS bucket.</p> required <code>prefix</code> <code>str</code> <p>Prefix path under which prediction JSON files are stored.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with columns 'tweet' and 'label' from all valid JSON files.</p> Source code in <code>src/mlops_hatespeech/drift_detector.py</code> <pre><code>def download_predictions_from_gcs(bucket_name: str, prefix: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Downloads prediction JSON files from a Google Cloud Storage bucket.\n\n    Each JSON is expected to contain a 'tweet' (under 'input_text') and a 'label' (under 'prediction').\n    These are collected into a DataFrame for further processing.\n\n    Args:\n        bucket_name (str): Name of the GCS bucket.\n        prefix (str): Prefix path under which prediction JSON files are stored.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns 'tweet' and 'label' from all valid JSON files.\n    \"\"\"\n    client = storage.Client()\n    bucket = client.bucket(bucket_name)\n    blobs = list(bucket.list_blobs(prefix=prefix))\n\n    rows = []\n    for blob in blobs:\n        if blob.name.endswith(\".json\"):\n            content = blob.download_as_string()\n            data = json.loads(content)\n            rows.append({\"tweet\": str(data.get(\"input_text\", \"\")), \"label\": data.get(\"prediction\", \"\")})\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"source/docs/#upload_report_to_gcs","title":"upload_report_to_gcs","text":"<p>Uploads a local file (e.g. an HTML report) to a specified location in a GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>str</code> <p>Path to the local file to be uploaded.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the target GCS bucket.</p> required <code>destination_path</code> <code>str</code> <p>Path (including filename) in the bucket where the file should be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/mlops_hatespeech/drift_detector.py</code> <pre><code>def upload_report_to_gcs(local_path: str, bucket_name: str, destination_path: str) -&gt; None:\n    \"\"\"\n    Uploads a local file (e.g. an HTML report) to a specified location in a GCS bucket.\n\n    Args:\n        local_path (str): Path to the local file to be uploaded.\n        bucket_name (str): Name of the target GCS bucket.\n        destination_path (str): Path (including filename) in the bucket where the file should be stored.\n\n    Returns:\n        None\n    \"\"\"\n    client = storage.Client()\n    bucket = client.bucket(bucket_name)\n    blob = bucket.blob(destination_path)\n    blob.upload_from_filename(local_path)\n    print(f\"Drift report uploaded to gs://{bucket_name}/{destination_path}\")\n</code></pre>"},{"location":"source/docs/#main_1","title":"main","text":"<p>Main routine for detecting embedding-based data drift on tweet inputs using BERT embeddings and uploading the Evidently report to GCS.</p> Source code in <code>src/mlops_hatespeech/drift_detector.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main routine for detecting embedding-based data drift on tweet inputs\n    using BERT embeddings and uploading the Evidently report to GCS.\n    \"\"\"\n    # Load reference dataset\n    dataset = load_from_disk(\"data/processed\")\n    reference_df = dataset[\"train\"].to_pandas()[[\"tweet\", \"label\"]].copy()\n    reference_df[\"tweet\"] = reference_df[\"tweet\"].astype(str)\n    # Load current predictions from GCS\n    current_df = download_predictions_from_gcs(BUCKET_NAME, PREDICTIONS_PREFIX)\n    current_df[\"tweet\"] = current_df[\"tweet\"].astype(str)\n    current_df[\"label\"] = current_df[\"label\"].astype(str).str.strip().str.lower().map({\"non-hate\": 0, \"hate\": 1})\n\n    if current_df.empty:\n        print(\"No predictions found in the bucket.\")\n        return\n\n    reference_texts = [str(t) for t in reference_df[\"tweet\"].tolist()]\n    current_texts = [str(t) for t in current_df[\"tweet\"].tolist()]\n\n    # Generate embeddings\n    reference_embeddings = get_bert_embeddings(reference_texts, tokenizer, model)\n    current_embeddings = get_bert_embeddings(current_texts, tokenizer, model)\n    reference_embed_df = pd.DataFrame(\n        reference_embeddings, columns=[f\"dim_{i}\" for i in range(reference_embeddings.shape[1])]\n    )\n    reference_embed_df[\"label\"] = reference_df[\"label\"].values\n\n    current_embed_df = pd.DataFrame(\n        current_embeddings, columns=[f\"dim_{i}\" for i in range(current_embeddings.shape[1])]\n    )\n    current_embed_df[\"label\"] = current_df[\"label\"].values\n    reference_embed_df[\"tweet\"] = reference_df[\"tweet\"].values\n    current_embed_df[\"tweet\"] = current_df[\"tweet\"].values\n\n    reference_embed_df[\"embedding_mean\"] = reference_embeddings.mean(axis=1)\n    current_embed_df[\"embedding_mean\"] = current_embeddings.mean(axis=1)\n\n    # Only pick relevant columns\n    reference_embed_df = reference_embed_df[[\"tweet\", \"label\", \"embedding_mean\"]]\n    current_embed_df = current_embed_df[[\"tweet\", \"label\", \"embedding_mean\"]]\n\n    # Drift score &gt; 70% to be categorized as data drift\n    report = Report(metrics=[DataDriftPreset(threshold=0.7)])\n    # generate report\n    eval = report.run(reference_data=reference_embed_df, current_data=current_embed_df)\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\") as tmp:\n        eval.save_html(tmp.name)\n        timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n        gcs_report_path = REPORT_OUTPUT_PATH.format(timestamp=timestamp)\n        upload_report_to_gcs(tmp.name, BUCKET_NAME, gcs_report_path)\n</code></pre>"}]}